<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Automated Visual Navigation Testing Pipeline | Adam D. Aufderheide </title> <meta name="author" content="Adam D. Aufderheide"> <meta name="description" content="MATLAB–Python–Blender pipeline for rendering, data association, and evaluation of visual navigation algorithms"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9A%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adamaufdi.github.io/projects/2_project/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Adam</span> D. Aufderheide </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Automated Visual Navigation Testing Pipeline</h1> <p class="post-description">MATLAB–Python–Blender pipeline for rendering, data association, and evaluation of visual navigation algorithms</p> </header> <article> <h2 id="project-overview">Project Overview</h2> <p>This project developed an <strong>automated testing pipeline for visual navigation algorithms</strong>, designed to evaluate robustness against the non-linear, non-Gaussian errors introduced during <strong>data association</strong>. The work was conducted in the <strong>Sensing, Controls, and Probabilistic Estimation (SCOPE) Lab</strong> and targeted navigation scenarios where real imagery of the environment is unavailable or impractical to collect, such as extraterrestrial terrain.</p> <p>The pipeline integrates <strong>MATLAB, Python, and Blender</strong> into a single automated workflow. MATLAB drives the navigation and estimation algorithms, Blender generates physically realistic rendered imagery from specified camera poses, and Python performs feature detection and data association. The system extends existing MATLAB-based visual navigation tools by enabling large-scale, repeatable testing using rendered image data.</p> <p>Role:<br> <strong>Adam Aufderheide</strong> – Pipeline architecture design, MATLAB–Python–Blender integration, automation, and system validation.</p> <h2 id="status-complete-demonstrated-under-and-reliable-when-tested-across-various-data-sets">Status: <strong>Complete</strong>, demonstrated under and reliable when tested across various data sets.</h2> <h2 id="visual-navigation-and-data-association">Visual Navigation and Data Association</h2> <p>Visual navigation algorithms estimate a camera’s <strong>pose (position and orientation)</strong> from images of the surrounding environment. A typical processing pipeline includes:</p> <ol> <li>Capturing an image of the environment</li> <li>Detecting visual features (e.g., ORB, SIFT, SURF)</li> <li> <strong>Associating detected features</strong> with a known map or database</li> <li>Feeding labeled feature coordinates into a navigation estimator</li> </ol> <p>Step 3—<strong>data association</strong>—is a critical failure point. Classical estimators such as the Extended Kalman Filter often assume perfect association, an assumption that can lead to estimator divergence when violated. Because these errors are difficult to model analytically, rigorous testing with real or realistically rendered imagery is essential prior to deployment.</p> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/8-480.webp 480w,/assets/img/8-800.webp 800w,/assets/img/8-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/8.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Visual navigation and data association concept" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/7-480.webp 480w,/assets/img/7-800.webp 800w,/assets/img/7-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Feature detection and labeling example" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Conceptual overview of visual navigation, feature detection, and data association using rendered imagery. </div> <hr> <h2 id="matlabpythonblender-pipeline-architecture">MATLAB–Python–Blender Pipeline Architecture</h2> <p>When real imagery is unavailable, the pipeline uses <strong>Blender</strong> to render images of the expected environment. Blender was chosen due to its open-source nature, ability to run without high-end GPUs, and its built-in <strong>Python API</strong>, which allows full programmatic control of rendering.</p> <p>The three-program architecture was selected for the strengths of each tool:</p> <ul> <li> <strong>MATLAB</strong>: navigation algorithms, estimation, and experiment orchestration</li> <li> <strong>Blender</strong>: image rendering from specified camera poses</li> <li> <strong>Python</strong>: feature detection and data association using modern computer vision libraries</li> </ul> <p>Communication between programs is handled using <strong>socket servers</strong>, enabling two-way messaging and allowing the pipeline to scale across multiple machines if needed.</p> <p>The automated process proceeds as follows:</p> <ol> <li>MATLAB sends a pose vector to Blender via Python</li> <li>Blender sets the camera pose and renders an image</li> <li>Python detects and associates features in the rendered image</li> <li>Labeled feature pixel coordinates are returned to MATLAB</li> </ol> <p>This closed-loop design enables large-scale, repeatable testing of navigation algorithms under controlled conditions.</p> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/9-480.webp 480w,/assets/img/9-800.webp 800w,/assets/img/9-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="MATLAB–Python–Blender pipeline diagram" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> High-level architecture of the automated MATLAB–Python–Blender visual navigation testing pipeline. </div> <hr> <h2 id="pipeline-implementation-and-sample-run">Pipeline Implementation and Sample Run</h2> <p>The pipeline is distributed as a repository containing:</p> <ul> <li>A MATLAB script for pose input and navigation processing</li> <li>A Blender <code class="language-plaintext highlighter-rouge">.blend</code> file containing the environment model and embedded Python API script</li> <li>Python-based feature detection and data association routines</li> </ul> <p>During execution, Blender runs continuously in a listening state while MATLAB sends pose vectors for rendering. Rendered images are saved automatically and passed to Python for processing. Status updates and file paths are communicated back to MATLAB, allowing seamless integration into existing workflows.</p> <p>A typical sample run demonstrates:</p> <ul> <li>MATLAB-defined pose input</li> <li>Blender-rendered imagery of the environment</li> <li>Feature detection and labeling overlaid on the rendered image</li> </ul> <h2 id="this-workflow-enables-systematic-testing-across-large-pose-sets-without-manual-intervention">This workflow enables systematic testing across large pose sets without manual intervention.</h2> <h2 id="applications-and-impact">Applications and Impact</h2> <p>Although initially developed for aerospace navigation and extraterrestrial terrain analysis, this pipeline has broad applicability across:</p> <ul> <li>Robotics and autonomous vehicles</li> <li>Visual SLAM and vSLAM research</li> <li>Simulation-based algorithm validation</li> <li>Perception system stress testing</li> </ul> <p>By automating image generation and data association, the pipeline enables deeper insight into estimator performance and failure modes, supporting more robust autonomy system development.</p> <hr> <h2 id="outcome">Outcome</h2> <p>This project delivered a <strong>fully automated, extensible testing pipeline</strong> for visual navigation algorithms, bridging MATLAB-based estimation frameworks with modern rendering and computer vision tools. The system significantly reduces the barrier to large-scale testing and provides a flexible foundation for future research in autonomy, robotics, and aerospace navigation.</p> </article> <div id="giscus_thread"> <script defer src="/assets/js/giscus-setup.js"></script> <noscript> Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Adam D. Aufderheide. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>